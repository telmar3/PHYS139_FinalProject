{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90299e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32768, 21)\n",
      "[[[ 3.45925393e-04 -3.47274076e-03 -5.37202061e-09 ...  1.43218656e-07\n",
      "    3.05353183e-07 -1.73395369e-02]\n",
      "  [-1.85789104e-04  1.86412979e-03  2.89339286e-09 ... -7.70794912e-08\n",
      "   -1.63774715e-07  2.44587902e-02]\n",
      "  [ 2.23084207e-04 -2.24002148e-03 -3.46064133e-09 ...  9.22875074e-08\n",
      "    1.97023908e-07 -4.45013773e-03]\n",
      "  ...\n",
      "  [ 4.69461447e-05 -1.31375040e-04 -3.40881057e-09 ...  1.60314301e-07\n",
      "    4.70688271e-08  1.69079576e-03]\n",
      "  [ 4.70481391e-05 -1.33088732e-04 -3.40860762e-09 ...  1.60324390e-07\n",
      "    4.71100492e-08  4.89403168e-03]\n",
      "  [ 4.71515705e-05 -1.34792994e-04 -3.40838890e-09 ...  1.60334196e-07\n",
      "    4.71511754e-08  8.08258727e-03]]\n",
      "\n",
      " [[ 7.89340760e-04  2.28750007e-03 -6.07063866e-09 ...  1.69187103e-07\n",
      "   -3.56135388e-07  1.31263509e-01]\n",
      "  [-4.24342172e-04 -1.23217807e-03  3.26501004e-09 ... -9.08889959e-08\n",
      "    1.91010585e-07 -8.12526345e-02]\n",
      "  [ 5.08856552e-04  1.47352973e-03 -3.91282029e-09 ...  1.09098877e-07\n",
      "   -2.29791240e-07  7.76870251e-02]\n",
      "  ...\n",
      "  [-9.81772610e-05  2.74828839e-04 -1.76006554e-09 ... -1.35397578e-08\n",
      "   -2.08337969e-08 -1.16966665e-03]\n",
      "  [-9.82167185e-05  2.74409278e-04 -1.75887771e-09 ... -1.35509630e-08\n",
      "   -2.08367545e-08 -4.37904662e-03]\n",
      "  [-9.82569036e-05  2.73984770e-04 -1.75768911e-09 ... -1.35620599e-08\n",
      "   -2.08394493e-08 -7.56244408e-03]]\n",
      "\n",
      " [[-3.59151047e-04  1.00497007e-02 -1.08526104e-08 ...  2.02919352e-08\n",
      "   -3.43831026e-08  1.48521692e-01]\n",
      "  [ 1.92586725e-04 -5.39693795e-03  5.82865578e-09 ... -1.05753095e-08\n",
      "    1.83534290e-08 -8.51877481e-02]\n",
      "  [-2.31756101e-04  6.48126565e-03 -6.99882241e-09 ...  1.32360407e-08\n",
      "   -2.22260166e-08  9.12172049e-02]\n",
      "  ...\n",
      "  [ 1.24705057e-05  9.07891372e-05  1.64158109e-09 ... -4.20842454e-08\n",
      "    3.83599152e-09  2.29685865e-02]\n",
      "  [ 1.24175695e-05  9.14955162e-05  1.64105052e-09 ... -4.21298871e-08\n",
      "    3.79180998e-09  1.96910556e-02]\n",
      "  [ 1.23644959e-05  9.21987448e-05  1.64054093e-09 ... -4.21755537e-08\n",
      "    3.74754094e-09  1.62022859e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.33901267e-04  4.49040486e-03  8.30981506e-09 ... -2.15355499e-07\n",
      "    7.99550151e-08  6.28175512e-02]\n",
      "  [ 2.34238774e-04 -2.41499208e-03 -4.47428938e-09 ...  1.16260644e-07\n",
      "   -4.25759126e-08 -1.98456272e-02]\n",
      "  [-2.79265689e-04  2.89431005e-03  5.35375522e-09 ... -1.38605216e-07\n",
      "    5.17328473e-08  4.62838560e-02]\n",
      "  ...\n",
      "  [-1.41168130e-04  3.63809377e-04  1.82052026e-10 ...  4.82267595e-08\n",
      "    5.85518691e-08  3.07876486e-02]\n",
      "  [-1.40968536e-04  3.64923791e-04  1.81186482e-10 ...  4.82856031e-08\n",
      "    5.86627280e-08  3.18654850e-02]\n",
      "  [-1.40769480e-04  3.66036402e-04  1.80324283e-10 ...  4.83448304e-08\n",
      "    5.87733879e-08  3.26607823e-02]]\n",
      "\n",
      " [[ 3.61482875e-04 -1.86954672e-03  3.04465142e-09 ...  1.49851743e-07\n",
      "    4.40873009e-07 -6.37648404e-02]\n",
      "  [-1.94330714e-04  1.00909849e-03 -1.63018021e-09 ... -8.06989107e-08\n",
      "   -2.37115188e-07  5.02875969e-02]\n",
      "  [ 2.33033265e-04 -1.20335759e-03  1.96581373e-09 ...  9.65395941e-08\n",
      "    2.84164912e-07 -3.38574164e-02]\n",
      "  ...\n",
      "  [ 5.04984855e-05 -4.32000787e-04  1.29233202e-09 ... -2.56412687e-08\n",
      "   -5.55976420e-08  8.82503297e-03]\n",
      "  [ 5.05376011e-05 -4.32544010e-04  1.29363009e-09 ... -2.56718753e-08\n",
      "   -5.55923911e-08  1.19922515e-02]\n",
      "  [ 5.05757380e-05 -4.33089590e-04  1.29490474e-09 ... -2.57025370e-08\n",
      "   -5.55874955e-08  1.51185049e-02]]\n",
      "\n",
      " [[-1.04164286e-03  1.60191394e-02  6.56576127e-10 ... -1.43388945e-07\n",
      "    9.36923016e-07 -4.29630615e-02]\n",
      "  [ 5.60858054e-04 -8.61598924e-03 -3.40978246e-10 ...  7.71964537e-08\n",
      "   -5.03929982e-07  3.88127640e-02]\n",
      "  [-6.71096961e-04  1.03249233e-02  4.28806768e-10 ... -9.23855055e-08\n",
      "    6.03879812e-07 -2.07678564e-02]\n",
      "  ...\n",
      "  [ 9.40077152e-05  1.42708421e-03  1.80304649e-09 ... -2.91634379e-08\n",
      "   -5.31469846e-09  1.97582971e-02]\n",
      "  [ 9.42004990e-05  1.42628886e-03  1.80261461e-09 ... -2.92345028e-08\n",
      "   -5.29094146e-09  2.25068145e-02]\n",
      "  [ 9.43924533e-05  1.42548617e-03  1.80218229e-09 ... -2.93056637e-08\n",
      "   -5.26703436e-09  2.49842685e-02]]]\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.data_preprocessing import send_freq_data, send_time_data\n",
    "\n",
    "freq_witness, strain_data = send_freq_data()\n",
    "time_witness, strain_data = send_time_data()\n",
    "\n",
    "print(freq_witness.shape)\n",
    "print(freq_witness)\n",
    "#print(strain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579177aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32768, 21)\n",
      "(32768, 21)\n",
      "[[ 3.4592539e-04 -3.4727408e-03 -5.3720206e-09 ...  1.4321866e-07\n",
      "   3.0535318e-07 -1.7339537e-02]\n",
      " [-1.8578910e-04  1.8641298e-03  2.8933929e-09 ... -7.7079491e-08\n",
      "  -1.6377471e-07  2.4458790e-02]\n",
      " [ 2.2308421e-04 -2.2400215e-03 -3.4606413e-09 ...  9.2287507e-08\n",
      "   1.9702391e-07 -4.4501377e-03]\n",
      " ...\n",
      " [ 4.6946145e-05 -1.3137504e-04 -3.4088106e-09 ...  1.6031430e-07\n",
      "   4.7068827e-08  1.6907958e-03]\n",
      " [ 4.7048139e-05 -1.3308873e-04 -3.4086076e-09 ...  1.6032439e-07\n",
      "   4.7110049e-08  4.8940317e-03]\n",
      " [ 4.7151570e-05 -1.3479299e-04 -3.4083889e-09 ...  1.6033420e-07\n",
      "   4.7151175e-08  8.0825873e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.034083776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimension 1 = batch\n",
    "#Dimension 2 = datapoint\n",
    "#Dimension 3 = witness channel\n",
    "\n",
    "print(freq_witness.shape)\n",
    "print(freq_witness[0].shape)\n",
    "print(freq_witness[0])\n",
    "freq_witness[0].max()\n",
    "freq_witness[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec434b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29491, 21) (3277, 21)\n"
     ]
    }
   ],
   "source": [
    "#split the time data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_time, test_time = train_test_split(\n",
    "    time_witness[0], test_size = 0.1, random_state = 42)\n",
    "\n",
    "print(train_time.shape, test_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af80293",
   "metadata": {},
   "source": [
    "# Attempt 3 (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "658483c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"attempt_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_45 (Conv1D)          (None, 29491, 21)         2226      \n",
      "                                                                 \n",
      " batch_normalization_81 (Bat  (None, 29491, 21)        84        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_46 (Conv1D)          (None, 14746, 8)          848       \n",
      "                                                                 \n",
      " batch_normalization_82 (Bat  (None, 14746, 8)         32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_47 (Conv1D)          (None, 7373, 16)          656       \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 7373, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_48 (Conv1D)          (None, 3687, 32)          2592      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 3687, 32)         128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_49 (Conv1D)          (None, 1844, 64)          10304     \n",
      "                                                                 \n",
      " batch_normalization_85 (Bat  (None, 1844, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_36 (Conv1D  (None, 3688, 32)         10272     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_86 (Bat  (None, 3688, 32)         128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_37 (Conv1D  (None, 7376, 16)         2576      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_87 (Bat  (None, 7376, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_38 (Conv1D  (None, 14752, 8)         648       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 14752, 8)         32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_39 (Conv1D  (None, 29504, 21)        861       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 29504, 21)        84        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " output (Conv1D)             (None, 29504, 1)          106       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,961\n",
      "Trainable params: 31,525\n",
      "Non-trainable params: 436\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Attempting to run the full 3d dataset didn't work\n",
    "#Let's try taking a single batch and running it through\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Check input shape, given dataset is sampled at 4096Hz\n",
    "model = keras.models.Sequential(name=\"attempt_3\")\n",
    "#Convolution Layers\n",
    "model.add(layers.Conv1D(filters=21, kernel_size=5, strides=1, padding=\"same\", activation=\"tanh\",\\\n",
    "                        input_shape=(train_time.shape[0], train_time.shape[1])))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "#Deconvolution Layers\n",
    "model.add(layers.Conv1DTranspose(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=21, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=1, kernel_size=5, padding=\"same\", name = \"output\"))\n",
    "\n",
    "model.build((None, train_time.shape[0], train_time.shape[1]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be7a024d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.684299502637792\n"
     ]
    }
   ],
   "source": [
    "#define mean relative error loss function\n",
    "import scipy.integrate as integrate\n",
    "def mean_relative_error(freq_witness, )\n",
    "f2 = freq_witness[0].max()\n",
    "f1 = freq_witness[0].min()\n",
    "recip = 1 / (f2 - f1)\n",
    "integrand = np.sqrt(residual/strain_data)\n",
    "integrate.quad(integrand, f1, f2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb1d5db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 29491, 21) for input KerasTensor(type_spec=TensorSpec(shape=(None, 29491, 21), dtype=tf.float32, name='conv1d_35_input'), name='conv1d_35_input', description=\"created by layer 'conv1d_35_input'\"), but it was called on an input with incompatible shape (None, 29491).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'attempt_3' (type Sequential).\n    \n    Input 0 of layer \"conv1d_35\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 29491)\n    \n    Call arguments received by layer 'attempt_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 29491), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_147/295238714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m results = model.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/mvigil/.local/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'attempt_3' (type Sequential).\n    \n    Input 0 of layer \"conv1d_35\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, 29491)\n    \n    Call arguments received by layer 'attempt_3' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 29491), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "#Toy code, need to double check lr_schedule parameters with literature\n",
    "import numpy as np\n",
    "\n",
    "#train_time = train_time.transpose()\n",
    "#test_time = test_time.transpose()\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=100, decay_rate=0.8)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=[\"accuracy\"])\n",
    "\n",
    "results = model.fit(\n",
    "    train_time,\n",
    "    test_time,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1, min_lr=1e-5),\n",
    "        keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81beae19",
   "metadata": {},
   "source": [
    "# Attempt 2 (scrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aad5782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"attempt_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_15 (Conv1D)          (None, 32768, 21)         2226      \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 32768, 21)        84        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 16384, 8)          848       \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 16384, 8)         32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 8192, 16)          656       \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 8192, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 4096, 32)          2592      \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 4096, 32)         128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 2048, 64)          10304     \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 2048, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_12 (Conv1D  (None, 4096, 32)         10272     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 4096, 32)         128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_13 (Conv1D  (None, 8192, 16)         2576      \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 8192, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_14 (Conv1D  (None, 16384, 8)         648       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 16384, 8)         32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_transpose_15 (Conv1D  (None, 32768, 21)        861       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 32768, 21)        84        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " output (Conv1D)             (None, 32768, 1)          106       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,961\n",
      "Trainable params: 31,525\n",
      "Non-trainable params: 436\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Implement neural network.\n",
    "\n",
    "#TOY MODEL COPY-PASTED FROM HW3\n",
    "#MODEL FOR PART B, MODIFIED TO REPLICATE\n",
    "#FIGURE 7 IN THE SOURCE WORK\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Check input shape, given dataset is sampled at 4096Hz\n",
    "model = keras.models.Sequential(name=\"attempt_2\")\n",
    "#Convolution Layers\n",
    "model.add(layers.Conv1D(filters=21, kernel_size=5, strides=1, padding=\"same\", activation=\"tanh\",\\\n",
    "                        input_shape=(train_time.shape[1], train_time.shape[2])))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "#Deconvolution Layers\n",
    "model.add(layers.Conv1DTranspose(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=21, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=1, kernel_size=5, padding=\"same\", name = \"output\"))\n",
    "\n",
    "model.build((None, train_time.shape[1], train_time.shape[2]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0ef5e99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 25\n  y sizes: 4\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_147/1968148541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m results = model.fit(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1846\u001b[0m             )\n\u001b[1;32m   1847\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 25\n  y sizes: 4\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "#Toy code, need to double check lr_schedule parameters with literature\n",
    "import numpy as np\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=100, decay_rate=0.8)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=[\"accuracy\"])\n",
    "\n",
    "results = model.fit(\n",
    "    train_time,\n",
    "    test_time,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1, min_lr=1e-5),\n",
    "        keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60729e99",
   "metadata": {},
   "source": [
    "# Attempt 1 (scrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9854cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement neural network.\n",
    "\n",
    "#TOY MODEL COPY-PASTED FROM HW3\n",
    "#MODEL FOR PART B, MODIFIED TO REPLICATE\n",
    "#FIGURE 7 IN THE SOURCE WORK\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Check input shape, given dataset is sampled at 4096Hz\n",
    "model = keras.models.Sequential(name=\"attempt_1\")\n",
    "#Convolution Layers\n",
    "model.add(layers.Conv1D(filters=21, kernel_size=5, strides=1, padding=\"same\", activation=\"tanh\",\\\n",
    "                        input_shape=(X_batch.shape[1], X_batch.shape[2])))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "#Deconvolution Layers\n",
    "model.add(layers.Conv1DTranspose(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(filters=21, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=1, kernel_size=5, padding=\"same\", name = \"output\"))\n",
    "\n",
    "model.build((None, X_batch.shape[1], X_batch.shape[0]))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
