{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e5779f",
   "metadata": {},
   "source": [
    "This code represents an attempt to replicate the DeepClean 1D convolutional neural network to reduce the signal-to-noise ratio in gravitational waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given pseudocode to grab the .h5 data file\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "with h5py.File(\"final_project/deepclean-1251335314-4097.h5\", \"r\") as f:\n",
    "    X = []\n",
    "    for channel, timeseries in f.items():\n",
    "        if channel = \"H1:GDS-CALIB_STRAIN\":\n",
    "            y = timeseries\n",
    "        else:\n",
    "            X.append(timeseries)\n",
    "X = np.stack(X)\n",
    "\n",
    "# then to construct inputs/outputs, you just\n",
    "# slice windows from these two timeseries\n",
    "window_length = 8\n",
    "sample_rate = 4096\n",
    "window_size = window_length * sample_rate\n",
    "\n",
    "# sample a batch of random windows\n",
    "batch_size = 32\n",
    "X_batch, y_batch = []\n",
    "for i in range(batch_size):\n",
    "    idx = np.random.randint(X.shape[-1] - window_size)\n",
    "    X_batch.append(X[:, idx: idx + window_size])\n",
    "    y_batch.append(y[idx: idx + window_size])\n",
    "\n",
    "X_batch = np.stack(X_batch)\n",
    "y_batch = np.stack(y_batch)\n",
    "\n",
    "# this method of generating batches is likely\n",
    "# to be very CPU-bound, and won't be any good\n",
    "# in any real training scenario. Luckily if you're\n",
    "# using torch and you can\n",
    "# pip install ml4gw\n",
    "# we've built a set of utilities for making\n",
    "# iterating through timeseries data easier\n",
    "from ml4gw.dataloading import InMemoryDataset\n",
    "dataloader = InMemoryDataset(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    kernel_size=window_size,\n",
    "    batch_size=batch_size,\n",
    "    coincident=True,\n",
    "    shuffle=True\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    for X, y in dataloader:\n",
    "        do_some_training(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6383223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing:\n",
    "#Normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement neural network.\n",
    "\n",
    "#TOY MODEL COPY-PASTED FROM HW3\n",
    "#MODEL FOR PART B, MODIFIED TO REPLICATE\n",
    "#FIGURE 7 IN THE SOURCE WORK\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#Check input shape, given dataset is sampled at 4096Hz\n",
    "model = keras.models.Sequential(name=\"attempt_1\")\n",
    "#Convolution Layers\n",
    "model.add(layers.Conv1D(filters=21, kernel_size=5, strides=1, padding=\"same\", activation=\"tanh\", input_shape=(21, 8192)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=8, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "#Deconvolution Layers\n",
    "model.add(layers.Conv1DTranspose(, activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(, activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(, activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv1DTranspose(, activation=\"tanh\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.3, name=\"dropout_1\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\", name=\"dense_1\"))\n",
    "\n",
    "model.build((None, 21, 8192))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
